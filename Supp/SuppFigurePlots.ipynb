{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np \n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "def encode_seq(seq):\n",
    "    \"\"\"\n",
    "    returns an encoded sequence \n",
    "    \n",
    "    Args:\n",
    "        seq: 2000bp sequence\n",
    "    \n",
    "    Returns:\n",
    "        4 x 2000 np.array\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #encode the sequence\n",
    "    mydict = {'A': np.asarray([1, 0, 0, 0]), 'G': np.asarray([0, 1, 0, 0]),\n",
    "            'C': np.asarray([0, 0, 1, 0]), 'T': np.asarray([0, 0, 0, 1]),\n",
    "            'N': np.asarray([0, 0, 0, 0]), 'H': np.asarray([0, 0, 0, 0]),\n",
    "            'a': np.asarray([1, 0, 0, 0]), 'g': np.asarray([0, 1, 0, 0]),\n",
    "            'c': np.asarray([0, 0, 1, 0]), 't': np.asarray([0, 0, 0, 1]),\n",
    "            'n': np.asarray([0, 0, 0, 0]), '-': np.asarray([0, 0, 0, 0])}\n",
    "    \n",
    "\n",
    "    #each column is the encoding for each nucleotide in the original seq\n",
    "    seq_encoded = np.zeros((4, len(seq)))\n",
    "    for i in range(len(seq)):\n",
    "        #this implements the encoding\n",
    "        seq_encoded[:,i] = mydict[seq[i]]\n",
    "\n",
    "\n",
    "        \n",
    "    return torch.from_numpy(seq_encoded)\n",
    "\n",
    "\n",
    "def mutate_seq(seq):\n",
    "    \"\"\"\n",
    "    returns an encoded sequence and every possible mutation\n",
    "    \n",
    "    Args:\n",
    "        seq: 2000bp sequence (encoded)\n",
    "    \n",
    "    Returns:\n",
    "        6000bp mutations encoded to a 6000 x 4 x 2000 np.array\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #encode the sequence\n",
    "    mydict = {'A': np.asarray([1, 0, 0, 0]), 'G': np.asarray([0, 1, 0, 0]),\n",
    "            'C': np.asarray([0, 0, 1, 0]), 'T': np.asarray([0, 0, 0, 1]),\n",
    "            'N': np.asarray([0, 0, 0, 0]), 'H': np.asarray([0, 0, 0, 0]),\n",
    "            'a': np.asarray([1, 0, 0, 0]), 'g': np.asarray([0, 1, 0, 0]),\n",
    "            'c': np.asarray([0, 0, 1, 0]), 't': np.asarray([0, 0, 0, 1]),\n",
    "            'n': np.asarray([0, 0, 0, 0]), '-': np.asarray([0, 0, 0, 0])}\n",
    "    \n",
    "\n",
    "\n",
    "    seq_encoded = encode_seq(seq)\n",
    "    seq_encoded_tile = np.tile(seq_encoded, (8000, 1, 1)) #changes to 8000, 4, 2000\n",
    "    \n",
    "    for j in range(len(seq)):\n",
    "        #for each element in the original sequence, the next three \"layers\" of \n",
    "        #seq_encoded_tile is a mutation\n",
    "        i = j*4\n",
    "        seq_encoded_tile[i, :, j] = mydict[\"A\"]\n",
    "        seq_encoded_tile[i + 1, :, j] = mydict[\"G\"]\n",
    "        seq_encoded_tile[i + 2, :, j] = mydict[\"C\"]\n",
    "        seq_encoded_tile[i + 3, :, j] = mydict[\"T\"]\n",
    "        \n",
    "        \n",
    "    return torch.from_numpy(seq_encoded_tile).float()\n",
    "\n",
    "class LambdaBase(nn.Sequential):\n",
    "    def __init__(self, fn, *args):\n",
    "        super(LambdaBase, self).__init__(*args)\n",
    "        self.lambda_func = fn\n",
    "\n",
    "    def forward_prepare(self, input):\n",
    "        output = []\n",
    "        for module in self._modules.values():\n",
    "            output.append(module(input))\n",
    "        return output if output else input\n",
    "\n",
    "class Lambda(LambdaBase):\n",
    "    def forward(self, input):\n",
    "        return self.lambda_func(self.forward_prepare(input))\n",
    "    \n",
    "class Beluga(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Beluga, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(4,320,(1, 8)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(320,320,(1, 8)),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.MaxPool2d((1, 4),(1, 4)),\n",
    "                nn.Conv2d(320,480,(1, 8)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(480,480,(1, 8)),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.MaxPool2d((1, 4),(1, 4)),\n",
    "                nn.Conv2d(480,640,(1, 8)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(640,640,(1, 8)),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Dropout(0.5),\n",
    "                Lambda(lambda x: x.view(x.size(0),-1)),\n",
    "                nn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(67840,2003)),\n",
    "                nn.ReLU(),\n",
    "                nn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(2003,2002)),\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "\n",
    "class BelugaMultiplexer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BelugaMultiplexer, self).__init__()\n",
    "        \n",
    "        self.model_one = nn.Sequential(\n",
    "                nn.Conv1d(5,640, 8, padding = 3),\n",
    "                nn.BatchNorm1d(640),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(640,640, 8, padding = 4),\n",
    "                nn.BatchNorm1d(640),\n",
    "                nn.ReLU())\n",
    "        \n",
    "        self.model_two = nn.Sequential(\n",
    "             \n",
    "                nn.Conv1d(640,1280, 8, dilation=4, padding= 14),\n",
    "                nn.BatchNorm1d(1280),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(1280,1280, 8, dilation=4, padding= 14),\n",
    "                nn.BatchNorm1d(1280),\n",
    "                nn.ReLU())\n",
    "                \n",
    "        self.model_three = nn.Sequential(\n",
    "            \n",
    "                nn.Conv1d(1280,1280, 8, dilation=16, padding= 56),\n",
    "                nn.BatchNorm1d(1280),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(1280,1280, 8, dilation=16, padding= 56),\n",
    "                nn.BatchNorm1d(1280),\n",
    "                nn.ReLU())\n",
    "        \n",
    "        \n",
    "        self.model_four = nn.Sequential(\n",
    "                nn.Conv1d(1280, 1280, 8, dilation=64, padding= 224),\n",
    "                nn.BatchNorm1d(1280),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(1280,1280, 8, dilation=64, padding= 224),\n",
    "                nn.BatchNorm1d(1280),\n",
    "                nn.ReLU())\n",
    "        \n",
    "        \n",
    "        self.model_five = nn.Sequential(\n",
    "                nn.Conv1d(1280,1280, 8, dilation=16, padding= 56),\n",
    "                nn.BatchNorm1d(1280),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(1280,1280, 8, dilation=16, padding= 56),\n",
    "                nn.BatchNorm1d(1280),\n",
    "                nn.ReLU())\n",
    "        \n",
    "        \n",
    "        self.model_six = nn.Sequential(\n",
    "                nn.Conv1d(1280,1280, 8, dilation=4, padding= 14),\n",
    "                nn.BatchNorm1d(1280),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(1280,1280, 8, dilation=4, padding= 14),\n",
    "                nn.BatchNorm1d(1280),\n",
    "                nn.ReLU())\n",
    "            \n",
    "        self.model_final = nn.Sequential(               \n",
    "                nn.Conv1d(1280,8008, 1, dilation=1, padding=0),\n",
    "                nn.BatchNorm1d(8008),\n",
    "                nn.ReLU(), \n",
    "                nn.Conv1d(8008,8008, 1, dilation=1, padding=0))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        encoding = torch.cat((torch.arange(0,1,0.001), torch.arange(1,0, -0.001))).unsqueeze(0).to(device)\n",
    "        encoding = encoding.repeat(x.shape[0], 1, 1)\n",
    "        x = torch.cat((x.to(device), encoding), dim = 1)\n",
    "        layer_one = self.model_one(x)\n",
    "        layer_two = self.model_two(layer_one)\n",
    "        layer_three = self.model_three(layer_two)\n",
    "        layer_four = self.model_four(layer_three)\n",
    "        layer_five = self.model_five(layer_four)\n",
    "        layer_six= self.model_six(layer_three + layer_five)\n",
    "        final_out = self.model_final(layer_two + layer_six)        \n",
    "        final_out = torch.reshape(final_out , (final_out.shape[0], 2002, 4, 2000))\n",
    "     \n",
    "        return final_out\n",
    "    \n",
    "    \n",
    "def log_fold(alt, ref):\n",
    "    \"\"\"\n",
    "    Returns the log fold of a,b\n",
    "    \n",
    "    returns log(((alt+1e-6) * (1-ref+1e-6)) /((1-alt+1e-6) * (ref+1e-6)) \n",
    "    \"\"\"\n",
    "    e = 10**(-6)\n",
    "    top = (alt + e)*(1 - ref + e)\n",
    "    bot = (1 - alt + e) * (ref + e)\n",
    "    return np.log(top/bot)\n",
    "\n",
    "B = Beluga()\n",
    "B.load_state_dict(torch.load('../data/deepsea.beluga.pth'))\n",
    "B.eval().cuda()\n",
    "\n",
    "BM = BelugaMultiplexer()\n",
    "\n",
    "BM.load_state_dict(torch.load(\"../data/comparison/weights/BelugaMultiplexerWeights.pth\"))\n",
    "BM.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a31d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BE = Beluga()\n",
    "BE.load_state_dict(torch.load('../data/deepsea.beluga.pth'))\n",
    "BE.eval().cuda()\n",
    "\n",
    "BM = BelugaMultiplexer()\n",
    "BM.load_state_dict(torch.load(\"../data/comparison/weights/BelugaMultiplexerWeights.pth\"))\n",
    "BM.eval().cuda()\n",
    "\n",
    "\n",
    "\n",
    "x = torch.load('input_list').cuda()  # x[i] is a 4x2000 reference sequence\n",
    "y = torch.load(\"seq_list_strings\") #y[i] the un-encoded version of x[i], it is a 'agct' - string\n",
    "\n",
    "count = 1\n",
    "count2 = 2\n",
    "for q in range(0, 9):\n",
    "    ### Get Multiplexer Prediction on x[0]\n",
    "\n",
    "    multiplexer_pred = BM(x[q].unsqueeze(0).cuda()).detach().cpu() #output shape is [1, 2002, 4, 2000]\n",
    "\n",
    "\n",
    "    ###Get Beluga Prediction\n",
    "\n",
    "    #First get all possible mutations of y[0] and encode them, mutates is of shape [8k, 4, 2k]\n",
    "    mutates = mutate_seq(y[q])\n",
    "\n",
    "\n",
    "    batch_size = 96\n",
    "    beluga_output = []\n",
    "\n",
    "    for i in range(int(math.floor(8000/batch_size)) + 1):\n",
    "\n",
    "        #Create a batch of inputs\n",
    "        batched_input = mutates[i*batch_size : (i+1)*batch_size, :, :].cuda() \n",
    "\n",
    "\n",
    "        #Pass the batched_input through the Beluga model and append to array\n",
    "        beluga_output.append(BE(batched_input.cuda()).cpu().detach())   \n",
    "\n",
    "    #stack the alternative array\n",
    "    beluga_output_alt = torch.vstack(beluga_output)\n",
    "\n",
    "\n",
    "\n",
    "    #define reference \n",
    "    reference_prediction = BE(x[q].unsqueeze(0))\n",
    "\n",
    "    ###Find the log_fold change \n",
    "    Beluga_output = log_fold(beluga_output_alt.detach().cpu(), reference_prediction.detach().cpu())\n",
    "\n",
    "\n",
    "    A = multiplexer_pred[0].transpose(1,2).reshape(2002, 8000).T\n",
    "    B = Beluga_output\n",
    "\n",
    "\n",
    "\n",
    "    plt.rcParams['ps.fonttype'] = 42\n",
    "    \n",
    "    temp = seaborn.clustermap(A.T.numpy(), row_cluster=True, col_cluster=False,cmap = \"RdBu_r\", center=0, vmax = 1, vmin = -1, yticklabels=False, rasterized=True )\n",
    "    roworder = temp.dendrogram_row.reordered_ind\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    c = seaborn.heatmap(A.T[roworder], cmap = \"RdBu_r\", center=0, vmax = 1, vmin = -1, rasterized=True )\n",
    "    c.set(yticklabels=[])\n",
    "    c.set_xticks([0,4000,8000])\n",
    "    c.set_xticklabels([-1000, 0, 1000])\n",
    "    c.tick_params(left=False) \n",
    "\n",
    "\n",
    "    plt.ylabel(\"Chromatin Profiles\", fontsize = 12)\n",
    "    plt.xlabel(\"Sequence Position of Mutations\", fontsize = 12)\n",
    "\n",
    "    plt.savefig('cmap' + str(count) + '.pdf', bbox_inches = 'tight', format = \"pdf\", dpi = 600)  \n",
    "    plt.figure()\n",
    "\n",
    "\n",
    "\n",
    "    plt.rcParams['ps.fonttype'] = 42\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "\n",
    "    c = seaborn.heatmap(B.T[roworder], cmap = \"RdBu_r\", center=0, vmax = 1, vmin = -1, rasterized=True )\n",
    "    c.set(yticklabels=[])\n",
    "    c.set_xticks([0,4000,8000])\n",
    "    c.set_xticklabels([-1000, 0, 1000])\n",
    "    c.tick_params(left=False) \n",
    "\n",
    "\n",
    "    plt.ylabel(\"Chromatin Profiles\", fontsize = 12)\n",
    "    plt.xlabel(\"Sequence Position of Mutations\", fontsize = 12)\n",
    "    plt.savefig('cmap' + str(count2) + '.pdf', bbox_inches = 'tight', format = \"pdf\", dpi = 600)  \n",
    "    plt.figure()\n",
    "                \n",
    "    count += 2\n",
    "    count2 += 2\n",
    "    \n",
    "    print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee90021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
